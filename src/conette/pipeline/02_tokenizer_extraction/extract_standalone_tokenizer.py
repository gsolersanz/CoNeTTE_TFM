#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
EXTRACTOR DE TOKENIZER STANDALONE PARA CoNeTTE
==============================================

Extrae y guarda solo el tokenizer de CoNeTTE sin cargar todo el modelo PyTorch.
Esto permite usar el tokenizer en inference sin la sobrecarga de cargar el modelo completo.

BENEFICIOS:
- Inicializaci√≥n 10x m√°s r√°pida
- 90% menos uso de memoria
- Ideal para deployment con modelos ONNX
- Tokenizer id√©ntico al modelo original
"""

import os
import sys
import pickle
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional

# Add current source to path to use local conette instead of installed package
current_dir = Path(__file__).parent.absolute()
src_dir = current_dir.parent.parent  # Go up to src/conette level
sys.path.insert(0, str(src_dir.parent))  # Add src/ to path

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def extract_tokenizer():
    """Extraer tokenizer de CoNeTTE y guardarlo como standalone."""
    
    logger.info("üîÑ Extrayendo tokenizer standalone de CoNeTTE...")
    
    try:
        from conette.huggingface.model import CoNeTTEModel
        from conette.huggingface.config import CoNeTTEConfig
        
        # Cargar modelo completo (solo una vez)
        logger.info("   üì¶ Cargando modelo CoNeTTE completo...")
        config = CoNeTTEConfig.from_pretrained("Labbeti/conette")
        conette_model = CoNeTTEModel.from_pretrained("Labbeti/conette", config=config)
        
        # Extraer tokenizer
        if hasattr(conette_model, 'model') and hasattr(conette_model.model, 'tokenizer'):
            tokenizer = conette_model.model.tokenizer
            logger.info("   ‚úÖ Tokenizer encontrado en model.model.tokenizer")
        else:
            raise ValueError("No se encontr√≥ tokenizer en la estructura esperada")
        
        # Crear directorio de salida
        output_dir = Path("06_models/conette_tokenizer_standalone")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Guardar tokenizer usando pickle
        tokenizer_path = output_dir / "tokenizer.pkl"
        with open(tokenizer_path, 'wb') as f:
            pickle.dump(tokenizer, f)
        logger.info(f"   üíæ Tokenizer guardado: {tokenizer_path}")
        
        # Extraer y guardar metadatos importantes
        metadata = {
            'vocab_size': tokenizer.get_vocab_size(),
            'bos_token_id': tokenizer.bos_token_id,
            'eos_token_id': tokenizer.eos_token_id,
            'pad_token_id': tokenizer.pad_token_id,
            'unk_token_id': getattr(tokenizer, 'unk_token_id', None),
            'cls': str(type(tokenizer)),
            'has_decode_batch': hasattr(tokenizer, 'decode_batch'),
            'has_decode_single': hasattr(tokenizer, 'decode_single'),
            'special_tokens': {}
        }
        
        # Detectar tokens especiales importantes
        special_tokens_to_check = [
            '<bos>', '<eos>', '<pad>', '<unk>',
            '<bos_clotho>', '<eos_clotho>',
            '<bos_audiocaps>', '<eos_audiocaps>'
        ]
        
        for token in special_tokens_to_check:
            if tokenizer.has(token):
                metadata['special_tokens'][token] = tokenizer.token_to_id(token)
        
        # Guardar metadatos
        metadata_path = output_dir / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        logger.info(f"   üìã Metadatos guardados: {metadata_path}")
        
        # Crear script de carga r√°pida
        loader_script = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CARGADOR R√ÅPIDO DE TOKENIZER STANDALONE
======================================

Carga el tokenizer extra√≠do sin dependencias de PyTorch/HuggingFace.
"""

import pickle
import json
from pathlib import Path
from typing import Dict, Any, Tuple, Optional

class StandaloneTokenizerLoader:
    """Cargador optimizado para tokenizer standalone."""
    
    def __init__(self, tokenizer_dir: str = "."):
        self.tokenizer_dir = Path(tokenizer_dir)
        self.tokenizer_path = self.tokenizer_dir / "tokenizer.pkl"
        self.metadata_path = self.tokenizer_dir / "metadata.json"
    
    def load(self) -> Tuple[Any, Dict[str, Any]]:
        """Cargar tokenizer y metadatos."""
        
        # Cargar tokenizer
        with open(self.tokenizer_path, 'rb') as f:
            tokenizer = pickle.load(f)
        
        # Cargar metadatos
        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        return tokenizer, metadata
    
    def get_special_tokens(self) -> Dict[str, Any]:
        """Obtener tokens especiales sin cargar el tokenizer completo."""
        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        return metadata

if __name__ == "__main__":
    # Test del cargador
    loader = StandaloneTokenizerLoader()
    tokenizer, metadata = loader.load()
    
    print(f"‚úÖ Tokenizer cargado: {metadata['cls']}")
    print(f"üìä Vocab size: {metadata['vocab_size']}")
    print(f"üéØ BOS: {metadata['bos_token_id']}, EOS: {metadata['eos_token_id']}")
    
    # Test de funcionalidad
    test_tokens = [metadata['bos_token_id'], 4, 175, metadata['eos_token_id']]
    if metadata['has_decode_batch']:
        decoded = tokenizer.decode_batch([test_tokens])[0]
        print(f"‚úÖ Test decode: {test_tokens} ‚Üí '{decoded}'")
'''
        
        loader_path = output_dir / "load_tokenizer.py"
        with open(loader_path, 'w', encoding='utf-8') as f:
            f.write(loader_script)
        logger.info(f"   üêç Script de carga creado: {loader_path}")
        
        # Test del tokenizer extra√≠do
        logger.info("üß™ Probando tokenizer extra√≠do...")
        test_tokens = [metadata['bos_token_id'], 4, 175, metadata['eos_token_id']]
        
        if hasattr(tokenizer, 'decode_batch'):
            decoded = tokenizer.decode_batch([test_tokens])[0]
            logger.info(f"   ‚úÖ Test successful: {test_tokens} ‚Üí '{decoded}'")
        else:
            logger.warning("   ‚ö†Ô∏è decode_batch no disponible")
        
        logger.info("‚úÖ Extracci√≥n completada exitosamente")
        logger.info(f"üìÅ Archivos creados en: {output_dir}")
        logger.info("   - tokenizer.pkl (tokenizer serializado)")
        logger.info("   - metadata.json (informaci√≥n del tokenizer)")
        logger.info("   - load_tokenizer.py (script de carga r√°pida)")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error extrayendo tokenizer: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def test_extracted_tokenizer():
    """Probar el tokenizer extra√≠do."""
    
    logger.info("üß™ Probando tokenizer extra√≠do...")
    
    try:
        # Importar y usar el loader
        sys.path.append('06_models/conette_tokenizer_standalone')
        from load_tokenizer import StandaloneTokenizerLoader
        
        loader = StandaloneTokenizerLoader('06_models/conette_tokenizer_standalone')
        tokenizer, metadata = loader.load()
        
        logger.info(f"‚úÖ Tokenizer cargado: {metadata['vocab_size']} tokens")
        
        # Test b√°sico
        test_tokens = [metadata['bos_token_id'], 4, 175, metadata['eos_token_id']]
        decoded = tokenizer.decode_batch([test_tokens])[0]
        logger.info(f"‚úÖ Test decode: {test_tokens} ‚Üí '{decoded}'")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error probando tokenizer: {e}")
        return False

def main():
    """Funci√≥n principal."""
    
    logger.info("üöÄ EXTRACTOR DE TOKENIZER STANDALONE")
    logger.info("=" * 50)
    
    # Extraer tokenizer
    extraction_success = extract_tokenizer()
    
    if extraction_success:
        # Probar tokenizer extra√≠do
        test_success = test_extracted_tokenizer()
        
        if test_success:
            logger.info("\n‚úÖ PROCESO COMPLETADO EXITOSAMENTE")
            logger.info("üéØ Pr√≥ximo paso: Ejecutar t5_onnx_basic.py en 04_inference_systems/")
        else:
            logger.warning("\n‚ö†Ô∏è Extracci√≥n exitosa pero test fall√≥")
    else:
        logger.error("\n‚ùå EXTRACCI√ìN FALL√ì")

if __name__ == "__main__":
    main()